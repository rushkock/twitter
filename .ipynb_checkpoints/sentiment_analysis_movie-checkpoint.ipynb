{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Data Science - Sentiment Analysis on twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "#pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "S_DIR = r'C:\\Users\\ruche\\Downloads\\geotagged_tweets_20160812-0912.1000.jsons'\n",
    "\n",
    "tweets_data = []\n",
    "tweets_file = open(S_DIR, \"r\")\n",
    "\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "# This creates a python list of strings with json data in the string.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['created_at', 'id', 'id_str', 'text', 'source', 'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'entities', 'extended_entities', 'favorited', 'retweeted', 'possibly_sensitive', 'filter_level', 'lang', 'timestamp_ms'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the keys of the dataset collected from one tweet \n",
    "# This gives an overview of what information is in the dataset\n",
    "print(tweets_data[0].keys())\n",
    "\n",
    "len(tweets_data)\n",
    "#inspect the keys that may be necessary for sentiment analysis. Essentially the tweet is the most important part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# A function that extracts the hyperlinks from the tweet's content.\n",
    "def extract_link(text):\n",
    "    regex = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    match = re.search(regex, text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return ''\n",
    "\n",
    "# A function that checks whether a word is included in the tweet's content\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "    if match:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame()\n",
    "\n",
    "tweets['text'] =    list(map(lambda tweet: tweet['text'], tweets_data))\n",
    "tweets['lang'] =    list(map(lambda tweet: tweet['lang'], tweets_data))\n",
    "tweets['country'] = list(map(lambda tweet: tweet['place']['country'] if tweet['place'] != None else None, tweets_data))\n",
    "tweets['state'] = list(map(lambda tweet: tweet['place']['full_name'] if tweet['place'] != None else None, tweets_data))\n",
    "tweets['link'] = tweets['text'].apply(lambda tweet: extract_link(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list for the text that has been \"cleaned\"\n",
    "list_clean_text = []\n",
    "\n",
    "for i in tweets[\"text\"]:\n",
    "    # this removes the hyperlink\n",
    "    result = re.sub(r\"http\\S+\", \"\", i)\n",
    "    # this removes the people the text was directed to e.g. @HillaryClinton or @CNN\n",
    "    result2 = re.sub(r\"\\B@\\S+.\", \"\", result)\n",
    "    result3 = re.sub(r\"\\d+.\", \"\", result2)\n",
    "    list_clean_text.append(result3)\n",
    "\n",
    "# make a new column with the clean text\n",
    "tweets[\"clean_text\"] = list_clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lexicon Normalization\n",
    "#performing stemming and Lemmatization\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Create a set of stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(set([\"n\", \"u\", \"im\"]))\n",
    "\n",
    "# Create a set of punctuation words \n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "# This is the function makeing the lemmatization\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# In this function we perform the entire cleaning\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "# This is the clean corpus.\n",
    "doc_clean = [clean(doc) for doc in list_clean_text] \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import html.parser as HTMLParser# In Python 3.4+ import html \n",
    "import nltk\n",
    "\n",
    "# Tokenize each clean text\n",
    "tokenized_tweets = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for i in doc_clean:\n",
    "    tokens = tokenizer.tokenize(i)\n",
    "    tokenized_tweets.append(tokens)\n",
    "\n",
    "tweets[\"tokens\"] = tokenized_tweets\n",
    "# flatten the list of lists to combine all words in one list\n",
    "flattened_list_of_tokens = [y for x in tokenized_tweets for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this function extracts features from text\n",
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return (wordlist, word_features)\n",
    "\n",
    "# view most common features\n",
    "word_features = get_word_features(flattened_list_of_tokens)\n",
    "word_features[0].most_common(10)\n",
    "\n",
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "word_features[0].plot(30,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# A function that extracts which words exist in a text based on a list of words to which we compare.\n",
    "def word_feats(words):\n",
    "        return dict([(word, True) for word in words])\n",
    "\n",
    "# Get the negative reviews for movies    \n",
    "negids = movie_reviews.fileids('neg')\n",
    "\n",
    "# Get the positive reviews for movies\n",
    "posids = movie_reviews.fileids('pos')\n",
    "# Find the features that most correspond to negative reviews    \n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "# Find the features that most correspond to positive reviews\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "\n",
    "# We would only use 1500 instances to train on. The quarter of the reviews left is for testing purposes.\n",
    "negcutoff = int(len(negfeats)*3/4)\n",
    "poscutoff = int(len(posfeats)*3/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# Construct the training dataset containing 50% positive reviews and 50% negative reviews\n",
    "trainfeats = negfeats[:negcutoff] + posfeats[:poscutoff]\n",
    "\n",
    "# Construct the negative dataset containing 50% positive reviews and 50% negative reviews\n",
    "testfeats = negfeats[negcutoff:] + posfeats[poscutoff:]\n",
    "\n",
    "print ('train on %d instances, test on %d instances' % (len(trainfeats), len(testfeats)))\n",
    "\n",
    "# Train a NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "\n",
    "# Test the trained classifier and display the most informative features.\n",
    "print ('accuracy:', nltk.classify.util.accuracy(classifier, testfeats))\n",
    "\n",
    "# See most informative features of the model \n",
    "#print (classifier.show_most_informative_features(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on the tweets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A list of the classifications\n",
    "classification = []\n",
    "\n",
    "# classify each token as positive or negative\n",
    "for i in tweets[\"tokens\"]:\n",
    "    classification.append(classifier.classify(word_feats(i)))\n",
    "    \n",
    "# make a column of the sentiment/classification of the tweet according to the model\n",
    "tweets[\"classification\"] = classification\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform sentiment analysis with textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob as tb\n",
    "\n",
    "tweets['sentiment'] = tweets['tokens'].apply(lambda x: tb(\" \".join(x)).sentiment[0] )\n",
    "#tweets[['text','calc_sentiment']]\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the model from nltk with textblob\n",
    "index = 0\n",
    "counter_different_classif = 0\n",
    "counter_neutral = 0\n",
    "comparison = []\n",
    "for classification in tweets[\"classification\"]:\n",
    "   # print(tweets[\"sentiment\"][index])\n",
    "    if classification == \"neg\" and tweets[\"sentiment\"][index] < 0:\n",
    "        comparison.append(\"same negative\")\n",
    "    elif classification == \"neg\" and tweets[\"sentiment\"][index] > 0:\n",
    "        comparison.append(\"different neg : pos\")\n",
    "        counter_different_classif = counter_different_classif + 1\n",
    "    elif classification == \"pos\" and tweets[\"sentiment\"][index] > 0:\n",
    "        comparison.append(\"same positive\")\n",
    "    elif classification == \"pos\" and tweets[\"sentiment\"][index] < 0:\n",
    "        comparison.append(\"different pos : neg\")\n",
    "        counter_different_classif = counter_different_classif + 1\n",
    "    else:\n",
    "        comparison.append(\"different neutral\")\n",
    "        counter_neutral = counter_neutral + 1\n",
    "    index = index + 1\n",
    "tweets[\"comparison\"] = comparison\n",
    "print(f\"Out of {len(tweets)} tweets, the models had a different classification {counter_different_classif} times\")\n",
    "print(f\"Out of {len(tweets)} tweets, textblob found {counter_neutral} neutral tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = []\n",
    "neg_words = []\n",
    "index = 0\n",
    "for i in tweets[\"classification\"]:\n",
    "    if i == \"pos\":\n",
    "        pos_words.append(tweets[\"tokens\"][index])\n",
    "    else:\n",
    "        neg_words.append(tweets[\"tokens\"][index])\n",
    "    index = index + 1\n",
    "\n",
    "def flatten(a_list):\n",
    "    flattened_list = [y for x in a_list for y in x]\n",
    "    return flattened_list\n",
    "\n",
    "all_pos_words = flatten(pos_words)\n",
    "all_neg_words = flatten(neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def word_cloud(tweets):\n",
    "    #We generate the wordcloud using the series created and the mask \n",
    "    word_cloud = WordCloud(width=2000, height=1000, max_font_size=200, max_words=100).generate(tweets)\n",
    "\n",
    "    # plot the wordcloud\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(word_cloud, interpolation=\"hermite\")\n",
    "    plt.axis(\"off\")    \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud(\" \".join(all_pos_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_cloud(\" \".join(all_neg_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Per State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.groupby([\"state\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
